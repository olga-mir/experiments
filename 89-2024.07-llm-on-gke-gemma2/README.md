# LLM on GKE


# Cost approximation

Yet to be tested, this quote is generated by GPT:

1. Compute (VM) Cost
Instance Type: n1-standard-8 (8 vCPUs and 30 GB memory)
Approximate Cost: About $0.31 per hour (cost varies by region)
2. GPU Cost
GPU Type: NVIDIA Tesla T4
Approximate Cost: About $0.35 per hour (cost varies by region)
3. Ephemeral Storage Cost
Storage Type: Local SSD (Ephemeral storage on GCP typically involves local SSDs)
Cost: Around $0.08 per GB per month
80 GiB Requested: 80 Gi * $0.08 = $6.40 per month, or approximately $0.0088 per hour
4. GKE Cluster Management Fee
Management Fee: $0.10 per cluster per hour for zonal clusters
Total Cost Estimate Per Hour
Compute: $0.31
GPU: $0.35
Ephemeral Storage: Approximately $0.009
GKE Management Fee: $0.10
Total: Around $0.769 per hour

# Explore

vllm image size on disk
```
  images:
  - names:
    - us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve@sha256:14bc6c5d4faecba1ce15bd2dba6622334fe24d8875913fe108f5d8f277f09872
    - us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240220_0936_RC01
    sizeBytes: 11869885985
```

Downloaded files:
```
% ls -lh your_model_directory
  689B  config.json
  183B  generation_config.json
  4.5G  model-00001-of-00006.safetensors
  4.5G  model-00002-of-00006.safetensors
  4.5G  model-00003-of-00006.safetensors
  4.5G  model-00004-of-00006.safetensors
  4.5G  model-00005-of-00006.safetensors
  2.5G  model-00006-of-00006.safetensors
   23K  model.safetensors.index.json
  414B  special_tokens_map.json
  1.8M  tokenizer.json
  1.7K  tokenizer_config.json


 9.98G  model-00001-of-00002.safetensors
  3.5G  model-00002-of-00002.safetensors
```

# Logs

Nvidia driver installer logs:

```
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0100   611  100   611    0     0   282k      0 --:--:-- --:--:-- --:--:--  298k
GPU driver auto installation is disabled.
Waiting for GPU driver libraries to be available.
GPU driver is installed.
InitContainer succeeded. Start nvidia-gpu-device-plugin container.
```

log:
```
I0419 23:33:17.284664    8339 nvidia_gpu.go:75] device-plugin started
I0419 23:33:17.284726    8339 nvidia_gpu.go:82] Reading GPU config file: /etc/nvidia/gpu_config.json
I0419 23:33:17.284760    8339 nvidia_gpu.go:86] Failed to parse GPU config file /etc/nvidia/gpu_config.json: unable to read gpu config file /etc/nvidia/gpu_config.json: open /etc/nvidia/gpu_config.json: no such file or directory
I0419 23:33:17.284768    8339 nvidia_gpu.go:87] Falling back to default GPU config.
I0419 23:33:17.284777    8339 manager.go:116] There is no Xid config specified
I0419 23:33:17.284811    8339 nvidia_gpu.go:96] Using gpu config: { 0 { 0} []}
I0419 23:33:19.240009    8339 nvidia_gpu.go:127] Starting metrics server on port: 2112, endpoint path: /metrics, collection frequency: 30000
I0419 23:33:19.240039    8339 metrics.go:138] Starting metrics server
I0419 23:33:19.240077    8339 metrics.go:144] nvml initialized successfully. Driver version: 535.129.03
I0419 23:33:19.240105    8339 devices.go:113] Found 1 GPU devices
I0419 23:33:19.246814    8339 devices.go:125] Found device nvidia0 for metrics collection
I0419 23:33:19.246856    8339 health_checker.go:65] Starting GPU Health Checker
I0419 23:33:19.246874    8339 health_checker.go:68] Healthchecker receives device nvidia0, device {nvidia0 Healthy nil {} 0}+
I0419 23:33:19.246903    8339 health_checker.go:77] Found 1 GPU devices
I0419 23:33:19.247597    8339 health_checker.go:140] Found non-mig device nvidia0 for health monitoring. UUID: GPU-de294bc4-9392-02a8-538c-6f15364088e7
I0419 23:33:19.247641    8339 health_checker.go:113] Registering device /dev/nvidia0. UUID: GPU-de294bc4-9392-02a8-538c-6f15364088e7
I0419 23:33:19.248773    8339 manager.go:413] will use alpha API
I0419 23:33:19.248786    8339 manager.go:427] starting device-plugin server at: /device-plugin/nvidiaGPU-1713569599.sock
I0419 23:33:19.248905    8339 manager.go:454] device-plugin server started serving
I0419 23:33:19.252075    8339 manager.go:462] device-plugin registered with the kubelet
I0419 23:33:19.252239    8339 beta_plugin.go:40] device-plugin: ListAndWatch start
I0419 23:33:19.252272    8339 beta_plugin.go:138] ListAndWatch: send devices &ListAndWatchResponse{Devices:[]*Device{&Device{ID:nvidia0,Health:Healthy,Topology:nil,},},}
```


# References

* Google Blog blogpost [Gemma on Google Kubernetes Engine deep dive: New innovations to serve open generative AI models](https://cloud.google.com/blog/products/containers-kubernetes/serving-gemma-on-google-kubernetes-engine-deep-dive), April 11, 2024
  * Serving stacks diagram to help understand different components and their relationships

* [Google official tutorials deploying Gemma on GKE](https://cloud.google.com/kubernetes-engine/docs/tutorials/serve-gemma-gpu-vllm)
  * The blog post linked above gives a quick introduction to each stack. This tutorial colleciton walks through deploying each one of them, explore the doc tree on the left.
  * TPUs - Newly announced JetStream inference engine
  * GPUs
    * vLLM: OSS LLM serving framework. ([vllm repo](https://github.com/vllm-project/vllm)))
    * TGI - Text Generation Inference: OSS LLM serving framework from Hugging Face.
    * TensorRT-LLM.

* [GitHub GKE Samples AI-ML](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/tree/main/ai-ml)

* https://cloud.google.com/kubernetes-engine/docs/how-to/gpus
