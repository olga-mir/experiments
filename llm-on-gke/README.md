# LLM on GKE


# Cost approximation

Yet to be tested, this quote is generated by GPT:

1. Compute (VM) Cost
Instance Type: n1-standard-8 (8 vCPUs and 30 GB memory)
Approximate Cost: About $0.31 per hour (cost varies by region)
2. GPU Cost
GPU Type: NVIDIA Tesla T4
Approximate Cost: About $0.35 per hour (cost varies by region)
3. Ephemeral Storage Cost
Storage Type: Local SSD (Ephemeral storage on GCP typically involves local SSDs)
Cost: Around $0.08 per GB per month
80 GiB Requested: 80 Gi * $0.08 = $6.40 per month, or approximately $0.0088 per hour
4. GKE Cluster Management Fee
Management Fee: $0.10 per cluster per hour for zonal clusters
Total Cost Estimate Per Hour
Compute: $0.31
GPU: $0.35
Ephemeral Storage: Approximately $0.009
GKE Management Fee: $0.10
Total: Around $0.769 per hour



# References

* Google Blog blogpost [Gemma on Google Kubernetes Engine deep dive: New innovations to serve open generative AI models](https://cloud.google.com/blog/products/containers-kubernetes/serving-gemma-on-google-kubernetes-engine-deep-dive), April 11, 2024
  * Serving stacks diagram to help understand different components and their relationships

* [Google official tutorials deploying Gemma on GKE](https://cloud.google.com/kubernetes-engine/docs/tutorials/serve-gemma-gpu-vllm)
  * The blog post linked above gives a quick introduction to each stack. This tutorial colleciton walks through deploying each one of them, explore the doc tree on the left.
  * TPUs
    * Newly announced JetStream inference engine
  * GPUs
    * vLLM: OSS LLM serving framework. ([vllm repo](https://github.com/vllm-project/vllm)))
    * TGI - Text Generation Inference: OSS LLM serving framework from Hugging Face.
    * TensorRT-LLM.

* https://github.com/GoogleCloudPlatform/kubernetes-engine-samples/tree/main/ai-ml

* https://cloud.google.com/kubernetes-engine/docs/how-to/gpus
